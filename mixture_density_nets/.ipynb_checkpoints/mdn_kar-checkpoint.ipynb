{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture density networks (MDN)\n",
    "\n",
    "Interesting material:\n",
    "\n",
    "- Bishop pattern recognition and machine learning: Mixture Density Networks page 272\n",
    "- https://amjadmahayri.wordpress.com/2014/04/30/mixture-density-networks/\n",
    "- http://www.cedar.buffalo.edu/~srihari/CSE574/Chap5/\n",
    "- http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42020.pdf\n",
    "- http://eprints.aston.ac.uk/373/1/NCRG_94_004.pdf\n",
    "- Implementation numpy: https://github.com/karpathy/randomfun/blob/master/MixtureDensityNets.ipynb\n",
    "\n",
    "#### Draw neural net \n",
    "- https://gist.github.com/craffel/2d727968c3aaebd10359\n",
    "\n",
    "#### What is the advantage of a mixture density network ?\n",
    "\n",
    " Instead of using the NN to predict directly the targets, use it to predict the parameters of a conditional distribution over the targets (given the input).\n",
    " \n",
    "- **Mixture density networks learn for each input a conditional distribution over the target**, each has the same form, but with different parameters. \n",
    "\n",
    "For example, we can assume that the target has a Gaussian conditional distribution, and the network predicts the mean of this distribution (we assume here that the variance is independent of the input, but this assumption be relaxed).\n",
    "\n",
    "If we train the network with the mean-squared error loss function we get the same solution as when we train it with the negative log-likelihood of a Gaussian. One might hope that we learn a more complex (multi-modal) conditional distribution for the target. This is actually the goal of Mixture density networks!\n",
    "\n",
    "We want to model the conditional distribution as a mixture of Gaussians, where each Gaussian component parameters are dependent on the input, that is \n",
    "\n",
    "\\begin{equation}\n",
    "P(y^m \\mid x^m) = \\sum_{k=1}^K \\pi_k(x^m) \\mathcal{N} \\left( y^m \\mid \\mu_k(x^m) , \\sigma_k^2(x^m) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "This network has 3 types of outputs:\n",
    "\n",
    "- A mixing coefficient $\\pi_k$\n",
    "- The mean of the Gaussian component $\\mu_k$\n",
    "- The variance of the Gaussian component $\\sigma_k^2$\n",
    "\n",
    "One might think: we don’t have ground truths for those outputs $\\pi_k,\\mu_k, \\sigma_k^2$ , how could make the network learn them?! the answer is we don’t need ground truths, because the loss function we’re going to use is the negative log likelihood given the data, so we just update the parameters of the model as to minimize this loss function.\n",
    "\n",
    "#### Implementation of the mixture density network:\n",
    "\n",
    "Implementation of the mixture density network: https://github.com/aalmah/ift6266amjad/blob/master/experiments/mdn.py\n",
    "\n",
    "This implementation\n",
    "\n",
    "- Accepts multiple samples at once. Therefore Gaussian components are multivariate.\n",
    "- Supports mini batches of data.\n",
    "- $\\mu$ is a 3d tensor\n",
    "\n",
    "Notice that the output layer is a bit different with respect to an standard MLP. The standard MLP uses a matrix as output layer for any regression problem. This network has in the output layer\n",
    "\n",
    "- A tensor for $\\mu$\n",
    "- A matrix for $\\sigma^2$\n",
    "- A matrix for $\\pi$\n",
    "\n",
    "Notice that\n",
    "\n",
    "- The activation function for $\\mu$ is the same as the desired output.\n",
    "- The activation function for $\\sigma^2$ is a softplus.\n",
    "- The activation function for $\\pi$ is a softmax.\n",
    "\n",
    "#### About NANS and the LogSumExp trick\n",
    "\n",
    "A straight implementation of MDN would cause a lot of NaNs.\n",
    "**A very important issue when implementing MDN is that you have the log-sum-exp expression in the log likelihood**, which otherwise can be numerically unstable. This can be fixed using this trick (https://github.com/Theano/Theano/issues/1563).\n",
    "\n",
    " We have to use a **smaller initial learning rate than a the one I used in my previous MLP is important**, otherwise we would get NaN in the likelihood. \n",
    " \n",
    " With these two tricks, we don’t get any NaNs. For the RNADE paper trick, I tried multiplying the mean with the variance in the cost function, but this changes the gradients of the variance and it makes the performance worse. In addition, I didn’t find it helping at all.\n",
    " \n",
    " Multiplying the gradient of the \\mu directly with the \\sigma is a little tricky when you’re using Theano’s automatic differentiation, and that’s probably why when I checked the RNADE code I found that they’re computing the gradients without using Theano’s T.grad\n",
    " \n",
    " \n",
    " \n",
    "#### Experiments \n",
    "\n",
    "- We would like to compare the MDN model with a similar MLP, and we can compare them in terms of the mean negative log-likelihood (Mean NLL) and the MSE on the same set of validation set.\n",
    "\n",
    "\n",
    "- In order to compute the MSE for the MDN model, we need to sample from the target conditional distribution. We do that by doing the following for each input data point: \n",
    "    - we sample the component from the multinomial distribution over the components (parametrized by the mixing coefficients), which gives us a selected component, \n",
    "    - then sample the prediction from the selected Gaussian component. \n",
    "\n",
    "\n",
    "- Computing the log-likelihood of the MLP is easy, it’s just the log of a Gaussian, with the output of the network as the mean, and its variance is the maximum likelihood estimate from the data, which turns out to be the MSE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Code from A.Karpathy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fae6cf794d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1MEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae6efd0110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate some 1D regression data (reproducing Bishop book data, page 273). \n",
    "# Note that the P(y|x) is not a nice distribution. E.g. it has three modes for x ~= 0.5\n",
    "N = 200\n",
    "X = np.linspace(0,1,N)\n",
    "Y = X + 0.3 * np.sin(2*3.1415926*X) + np.random.uniform(-0.1, 0.2, N)\n",
    "X,Y = Y,X\n",
    "plt.scatter(X,Y,color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model intialization\n",
    "input_size = 1\n",
    "hidden_size = 30\n",
    "K = 4 # number of mixture components\n",
    "m = {}\n",
    "m['Wxh'] = np.random.randn(hidden_size, input_size) * 0.1 # input to hidden\n",
    "m['Whu'] = np.random.randn(K, hidden_size) * 0.1 # hidden to means\n",
    "m['Whs'] = np.random.randn(K, hidden_size) * 0.1 # hidden to log standard deviations\n",
    "m['Whp'] = np.random.randn(K, hidden_size) * 0.1 # hidden to mixing coefficients (cluster priors)\n",
    "m['bxh'] = np.random.randn(hidden_size, 1) * 0.01\n",
    "m['bhu'] = np.random.randn(K, 1) * 0.01\n",
    "m['bhs'] = np.random.randn(K, 1) * 0.01\n",
    "m['bhp'] = np.random.randn(K, 1) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # softmaxes the columns of x\n",
    "    #z = x - np.max(x, axis=0, keepdims=True) # for safety\n",
    "    e = np.exp(x)\n",
    "    en = e / np.sum(e, axis=0, keepdims=True)\n",
    "    return en\n",
    "\n",
    "def mdn_loss(x, y, m):\n",
    "    # data in X are columns\n",
    "    \n",
    "    # forward pass\n",
    "    h = np.tanh(np.dot(m['Wxh'], x) + m['bxh'])\n",
    "    \n",
    "    # predict mean\n",
    "    mu = np.dot(m['Whu'], h) + m['bhu']\n",
    "    \n",
    "    # predict log variance\n",
    "    logsig = np.dot(m['Whs'], h) + m['bhs']\n",
    "    sig = np.exp(logsig)\n",
    "    \n",
    "    # predict mixture priors\n",
    "    piu = np.dot(m['Whp'], h) + m['bhp'] # unnormalized pi\n",
    "    pi = softmax(piu)\n",
    "    \n",
    "    # compute the loss: mean negative data log likelihood\n",
    "    k,n = mu.shape # number of mixture components\n",
    "    ps = np.exp(-((y - mu)**2)/(2*sig**2))/(sig*np.sqrt(2*math.pi))\n",
    "    pin = ps * pi\n",
    "    lp = -np.log(np.sum(pin, axis=0, keepdims=True))\n",
    "    loss = np.sum(lp)/n\n",
    "    \n",
    "    # compute the gradients on nn outputsmi\n",
    "    grad = {}\n",
    "    gammas = pin / np.sum(pin, axis=0, keepdims = True)\n",
    "    dmu = gammas * ((mu - y)/sig**2) /n\n",
    "    dlogsig = gammas * (1.0 - (y-mu)**2/(sig**2)) /n\n",
    "    dpiu = (pi - gammas) /n\n",
    "    \n",
    "    # backprop to decoder matrices\n",
    "    grad['bhu'] = np.sum(dmu, axis=1, keepdims=True)\n",
    "    grad['bhs'] = np.sum(dlogsig, axis=1, keepdims=True)\n",
    "    grad['bhp'] = np.sum(dpiu, axis=1, keepdims=True)\n",
    "    grad['Whu'] = np.dot(dmu, h.T)\n",
    "    grad['Whs'] = np.dot(dlogsig, h.T)\n",
    "    grad['Whp'] = np.dot(dpiu, h.T)\n",
    "    \n",
    "    # backprop to h\n",
    "    dh = np.dot(m['Whu'].T, dmu) + np.dot(m['Whs'].T, dlogsig) + np.dot(m['Whp'].T, dpiu)\n",
    "    \n",
    "    # backprop tanh\n",
    "    dh = (1.0-h**2)*dh\n",
    "    \n",
    "    # backprop input to hidden\n",
    "    grad['bxh'] = np.sum(dh, axis=1, keepdims=True)\n",
    "    grad['Wxh'] = np.dot(dh, x.T)\n",
    "    \n",
    "    # misc stats\n",
    "    stats = {}\n",
    "    stats['lp'] = lp\n",
    "    return loss, grad, stats\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = N # full batch\n",
    "xbatch = np.reshape(X[:nb], (1,nb))\n",
    "ybatch = np.reshape(Y[:nb], (1,nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbatch.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ': ', 1.0716429390397726)\n",
      "(1000, ': ', -0.46805437819808804)\n",
      "(2000, ': ', -0.63902758613941757)\n",
      "(3000, ': ', -0.67943513467508609)\n",
      "(4000, ': ', -0.69586483732968807)\n",
      "(5000, ': ', -0.70352980721083513)\n",
      "(6000, ': ', -0.70773306395225943)\n",
      "(7000, ': ', -0.71058356659001121)\n",
      "(8000, ': ', -0.71291210268911587)\n",
      "(9000, ': ', -0.71507612555439837)\n",
      "(10000, ': ', -0.71725073600897071)\n",
      "(11000, ': ', -0.7195290131658042)\n",
      "(12000, ': ', -0.72195628435785619)\n",
      "(13000, ': ', -0.72452807716578205)\n",
      "(14000, ': ', -0.72721027648868641)\n",
      "(15000, ': ', -0.72986392459728922)\n",
      "(16000, ': ', -0.7324549280373952)\n",
      "(17000, ': ', -0.73486715956603832)\n",
      "(18000, ': ', -0.73704214029008597)\n",
      "(19000, ': ', -0.73897014605286304)\n"
     ]
    }
   ],
   "source": [
    "# optimize\n",
    "lr = 1e-2\n",
    "mem = {}\n",
    "for k in m.keys(): mem[k] = np.zeros_like(m[k]) # init adagrad\n",
    "for k in range(20000):\n",
    "    loss, grad, stats = mdn_loss(xbatch, ybatch, m)\n",
    "    if k % 1000 == 0:\n",
    "        print (k, \": \", loss)\n",
    "        \n",
    "    for k,v in grad.items():\n",
    "        mem[k] += grad[k]**2\n",
    "        m[k] += -lr * grad[k] / np.sqrt(mem[k] + 1e-8)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function for creating contour plot of the predictions\n",
    "def drawContour(m):\n",
    "    n = 50\n",
    "    xx = np.linspace(0,1,n)\n",
    "    yy = np.linspace(0,1,n)\n",
    "    xm, ym = np.meshgrid(xx, yy)\n",
    "    _, _, stats = mdn_loss(xm.reshape(1, xm.size), ym.reshape(1, ym.size), m)\n",
    "    logps = stats['lp']\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(X,Y,color='g')\n",
    "    lp = stats['lp']\n",
    "    plt.contour(xm, ym, np.reshape(logps, (n, n)), levels=np.linspace(lp.min(), lp.max(), 50))\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('3-component Gaussian Mixture Model for P(y|x)')\n",
    "#drawContour(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drawContour(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running gradient check...\n",
      "('checking gradient on parameter', 'bxh', 'of shape ...', '(30, 1)')\n",
      "OK bxh index       13 (val = +0.113087), analytic = -0.033412, numerical = -0.033412, rel error = +0.000000\n",
      "OK bxh index       10 (val = -0.157753), analytic = -0.002271, numerical = -0.002271, rel error = +0.000000\n",
      "OK bxh index       19 (val = -0.579510), analytic = +0.051569, numerical = +0.051569, rel error = +0.000000\n",
      "OK bxh index       16 (val = +0.106845), analytic = +0.025360, numerical = +0.025360, rel error = +0.000000\n",
      "OK bxh index       24 (val = +0.246396), analytic = +0.009760, numerical = +0.009760, rel error = +0.000000\n",
      "OK bxh index       15 (val = -0.341481), analytic = -0.038671, numerical = -0.038671, rel error = +0.000000\n",
      "OK bxh index        5 (val = +0.259327), analytic = -0.015310, numerical = -0.015310, rel error = +0.000000\n",
      "OK bxh index        0 (val = -0.113669), analytic = +0.041825, numerical = +0.041825, rel error = +0.000000\n",
      "OK bxh index       13 (val = +0.113087), analytic = -0.033412, numerical = -0.033412, rel error = +0.000000\n",
      "OK bxh index        3 (val = +0.102851), analytic = -0.015887, numerical = -0.015887, rel error = +0.000000\n",
      "('checking gradient on parameter', 'bhu', 'of shape ...', '(4, 1)')\n",
      "OK bhu index        1 (val = -0.060103), analytic = +0.039910, numerical = +0.039910, rel error = +0.000000\n",
      "OK bhu index        1 (val = -0.060103), analytic = +0.039910, numerical = +0.039910, rel error = +0.000000\n",
      "OK bhu index        3 (val = +0.239519), analytic = +0.001139, numerical = +0.001139, rel error = +0.000000\n",
      "OK bhu index        2 (val = +0.044756), analytic = +0.087753, numerical = +0.087753, rel error = +0.000000\n",
      "OK bhu index        1 (val = -0.060103), analytic = +0.039910, numerical = +0.039910, rel error = +0.000000\n",
      "OK bhu index        2 (val = +0.044756), analytic = +0.087753, numerical = +0.087753, rel error = +0.000000\n",
      "OK bhu index        1 (val = -0.060103), analytic = +0.039910, numerical = +0.039910, rel error = +0.000000\n",
      "OK bhu index        1 (val = -0.060103), analytic = +0.039910, numerical = +0.039910, rel error = +0.000000\n",
      "OK bhu index        0 (val = -0.053471), analytic = -0.183041, numerical = -0.183041, rel error = +0.000000\n",
      "OK bhu index        2 (val = +0.044756), analytic = +0.087753, numerical = +0.087753, rel error = +0.000000\n",
      "('checking gradient on parameter', 'Whu', 'of shape ...', '(4, 30)')\n",
      "OK Whu index       15 (val = +0.144196), analytic = +0.079693, numerical = +0.079693, rel error = +0.000000\n",
      "OK Whu index       63 (val = +0.238369), analytic = +0.013001, numerical = +0.013001, rel error = +0.000000\n",
      "OK Whu index       68 (val = -0.122596), analytic = +0.031838, numerical = +0.031838, rel error = +0.000000\n",
      "OK Whu index       69 (val = -0.193468), analytic = -0.034195, numerical = -0.034195, rel error = +0.000000\n",
      "OK Whu index       85 (val = +0.036259), analytic = +0.004795, numerical = +0.004795, rel error = +0.000000\n",
      "OK Whu index        7 (val = +0.068083), analytic = -0.004494, numerical = -0.004494, rel error = +0.000000\n",
      "OK Whu index      102 (val = -0.237443), analytic = +0.001143, numerical = +0.001143, rel error = +0.000000\n",
      "OK Whu index       74 (val = +0.019882), analytic = +0.027567, numerical = +0.027567, rel error = +0.000000\n",
      "OK Whu index       20 (val = -0.185972), analytic = +0.023529, numerical = +0.023529, rel error = +0.000000\n",
      "OK Whu index        6 (val = +0.030900), analytic = -0.073365, numerical = -0.073365, rel error = +0.000000\n",
      "('checking gradient on parameter', 'Whp', 'of shape ...', '(4, 30)')\n",
      "OK Whp index       96 (val = -0.340767), analytic = +0.000201, numerical = +0.000201, rel error = +0.000000\n",
      "OK Whp index       48 (val = +0.160289), analytic = +0.000048, numerical = +0.000048, rel error = +0.000000\n",
      "OK Whp index       43 (val = +0.248092), analytic = +0.000043, numerical = +0.000043, rel error = +0.000000\n",
      "OK Whp index       63 (val = -0.372267), analytic = -0.000162, numerical = -0.000162, rel error = +0.000000\n",
      "OK Whp index       91 (val = +0.844326), analytic = -0.000164, numerical = -0.000164, rel error = +0.000000\n",
      "OK Whp index        8 (val = -0.881996), analytic = -0.000246, numerical = -0.000246, rel error = +0.000000\n",
      "OK Whp index      115 (val = -0.065271), analytic = +0.000084, numerical = +0.000084, rel error = +0.000000\n",
      "OK Whp index       69 (val = -0.182360), analytic = +0.000188, numerical = +0.000188, rel error = +0.000000\n",
      "OK Whp index       66 (val = +0.603821), analytic = -0.000116, numerical = -0.000116, rel error = +0.000000\n",
      "OK Whp index       99 (val = +0.472409), analytic = -0.000210, numerical = -0.000210, rel error = +0.000000\n",
      "('checking gradient on parameter', 'Whs', 'of shape ...', '(4, 30)')\n",
      "OK Whs index        4 (val = -0.345800), analytic = -0.000286, numerical = -0.000286, rel error = +0.000000\n",
      "OK Whs index      114 (val = -0.284939), analytic = +0.000229, numerical = +0.000229, rel error = +0.000000\n",
      "OK Whs index       20 (val = +0.211603), analytic = +0.000129, numerical = +0.000129, rel error = +0.000000\n",
      "OK Whs index      102 (val = +0.168207), analytic = -0.000208, numerical = -0.000208, rel error = +0.000000\n",
      "OK Whs index       33 (val = -0.345511), analytic = +0.000014, numerical = +0.000014, rel error = +0.000000\n",
      "OK Whs index       14 (val = -0.590798), analytic = -0.000099, numerical = -0.000099, rel error = +0.000000\n",
      "OK Whs index       40 (val = +0.277479), analytic = -0.000020, numerical = -0.000020, rel error = +0.000000\n",
      "OK Whs index       53 (val = -0.269182), analytic = +0.000017, numerical = +0.000017, rel error = +0.000000\n",
      "OK Whs index       59 (val = -0.227226), analytic = +0.000026, numerical = +0.000026, rel error = +0.000000\n",
      "OK Whs index       61 (val = -0.054825), analytic = -0.000549, numerical = -0.000549, rel error = +0.000000\n",
      "('checking gradient on parameter', 'bhp', 'of shape ...', '(4, 1)')\n",
      "OK bhp index        2 (val = +0.771351), analytic = -0.000341, numerical = -0.000341, rel error = +0.000000\n",
      "OK bhp index        2 (val = +0.771351), analytic = -0.000341, numerical = -0.000341, rel error = +0.000000\n",
      "OK bhp index        0 (val = -0.456464), analytic = -0.000370, numerical = -0.000370, rel error = +0.000000\n",
      "OK bhp index        0 (val = -0.456464), analytic = -0.000370, numerical = -0.000370, rel error = +0.000000\n",
      "OK bhp index        1 (val = -0.079733), analytic = +0.000211, numerical = +0.000211, rel error = +0.000000\n",
      "OK bhp index        0 (val = -0.456464), analytic = -0.000370, numerical = -0.000370, rel error = +0.000000\n",
      "OK bhp index        2 (val = +0.771351), analytic = -0.000341, numerical = -0.000341, rel error = +0.000000\n",
      "OK bhp index        3 (val = -0.374979), analytic = +0.000500, numerical = +0.000500, rel error = +0.000000\n",
      "OK bhp index        2 (val = +0.771351), analytic = -0.000341, numerical = -0.000341, rel error = +0.000000\n",
      "OK bhp index        2 (val = +0.771351), analytic = -0.000341, numerical = -0.000341, rel error = +0.000000\n",
      "('checking gradient on parameter', 'bhs', 'of shape ...', '(4, 1)')\n",
      "OK bhs index        0 (val = -0.087106), analytic = -0.000418, numerical = -0.000418, rel error = +0.000000\n",
      "OK bhs index        0 (val = -0.087106), analytic = -0.000418, numerical = -0.000418, rel error = +0.000000\n",
      "OK bhs index        3 (val = -0.254398), analytic = +0.000641, numerical = +0.000641, rel error = +0.000000\n",
      "OK bhs index        2 (val = -0.413771), analytic = -0.000482, numerical = -0.000482, rel error = +0.000000\n",
      "OK bhs index        3 (val = -0.254398), analytic = +0.000641, numerical = +0.000641, rel error = +0.000000\n",
      "OK bhs index        2 (val = -0.413771), analytic = -0.000482, numerical = -0.000482, rel error = +0.000000\n",
      "OK bhs index        3 (val = -0.254398), analytic = +0.000641, numerical = +0.000641, rel error = +0.000000\n",
      "OK bhs index        1 (val = -0.268195), analytic = +0.000104, numerical = +0.000104, rel error = +0.000000\n",
      "OK bhs index        2 (val = -0.413771), analytic = -0.000482, numerical = -0.000482, rel error = +0.000000\n",
      "OK bhs index        3 (val = -0.254398), analytic = +0.000641, numerical = +0.000641, rel error = +0.000000\n",
      "('checking gradient on parameter', 'Wxh', 'of shape ...', '(30, 1)')\n",
      "OK Wxh index       15 (val = -0.126270), analytic = -0.027150, numerical = -0.027150, rel error = +0.000000\n",
      "OK Wxh index       19 (val = +0.594067), analytic = +0.031541, numerical = +0.031541, rel error = +0.000000\n",
      "OK Wxh index       18 (val = +0.297458), analytic = +0.013172, numerical = +0.013172, rel error = +0.000000\n",
      "OK Wxh index        6 (val = -0.048462), analytic = -0.001130, numerical = -0.001130, rel error = +0.000000\n",
      "OK Wxh index       17 (val = +0.110532), analytic = +0.005035, numerical = +0.005035, rel error = +0.000000\n",
      "OK Wxh index       28 (val = -0.531330), analytic = +0.032010, numerical = +0.032010, rel error = +0.000000\n",
      "OK Wxh index       11 (val = +0.003313), analytic = +0.028417, numerical = +0.028417, rel error = +0.000000\n",
      "OK Wxh index       19 (val = +0.594067), analytic = +0.031541, numerical = +0.031541, rel error = +0.000000\n",
      "OK Wxh index       21 (val = -0.325420), analytic = -0.002796, numerical = -0.002796, rel error = +0.000000\n",
      "OK Wxh index       10 (val = -0.202293), analytic = +0.010820, numerical = +0.010820, rel error = +0.000000\n"
     ]
    }
   ],
   "source": [
    "def wrapCost(batch, model):\n",
    "    loss, grad, stats = mdn_loss(batch[0], batch[1], model)\n",
    "    cg = {}\n",
    "    cg['grad'] = grad\n",
    "    cg['cost'] = {}\n",
    "    cg['cost']['total_cost'] = loss\n",
    "    return cg\n",
    "\n",
    "def gradCheck(batch, model, cost_function, **kwargs):\n",
    "    num_checks = kwargs.get('num_checks', 10)\n",
    "    delta = kwargs.get('delta', 1e-5)\n",
    "    rel_error_thr_warning = kwargs.get('rel_error_thr_warning', 1e-2)\n",
    "    rel_error_thr_error = kwargs.get('rel_error_thr_error', 1)\n",
    "\n",
    "    cg = cost_function(batch, model)\n",
    "\n",
    "    print ('running gradient check...')\n",
    "    for p in model.keys():\n",
    "      print('checking gradient on parameter',p,'of shape ...', str(model[p].shape))\n",
    "      mat = model[p]\n",
    "\n",
    "      s0 = cg['grad'][p].shape\n",
    "      s1 = mat.shape\n",
    "      #assert s0 == s1, 'Error dims dont match on %s: grad: %s vs. param: %s.' % (p, `s0`, `s1`)\n",
    "\n",
    "      for i in range(num_checks):\n",
    "        #ri = randi(mat.size)\n",
    "        ri = np.random.randint(0, mat.size)\n",
    "\n",
    "        # evluate cost at [x + delta] and [x - delta]\n",
    "        old_val = mat.flat[ri]\n",
    "        mat.flat[ri] = old_val + delta\n",
    "        cg0 = cost_function(batch, model)\n",
    "        mat.flat[ri] = old_val - delta\n",
    "        cg1 = cost_function(batch, model)\n",
    "        mat.flat[ri] = old_val # reset old value for this parameter\n",
    "\n",
    "        # fetch both numerical and analytic gradient\n",
    "        grad_analytic = cg['grad'][p].flat[ri]\n",
    "        grad_numerical = (cg0['cost']['total_cost'] - cg1['cost']['total_cost']) / ( 2 * delta )\n",
    "\n",
    "        # compare them\n",
    "        if grad_numerical == 0 and grad_analytic == 0:\n",
    "          rel_error = 0 # both are zero, OK.\n",
    "          status = 'OK'\n",
    "        elif abs(grad_numerical) < 1e-7 and abs(grad_analytic) < 1e-7:\n",
    "          rel_error = 0 # not enough precision to check this\n",
    "          status = 'VAL SMALL WARNING'\n",
    "        else:\n",
    "          rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + grad_analytic)\n",
    "          status = 'OK'\n",
    "          if rel_error > rel_error_thr_warning: status = 'WARNING'\n",
    "          if rel_error > rel_error_thr_error: status = '!!!!! NOTOK'\n",
    "\n",
    "        #print stats\n",
    "        print '%s %s index %8d (val = %+8f), analytic = %+8f, numerical = %+8f, rel error = %+8f' \\\n",
    "              % (status, p, ri, old_val, grad_analytic, grad_numerical, rel_error)\n",
    "            \n",
    "gradCheck((xbatch, ybatch), m, wrapCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
